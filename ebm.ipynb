{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f5575f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d6fe6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b66c03c90ce725319f0691dfb29845ac",
     "grade": false,
     "grade_id": "cell-9c430af5d8d383e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Deep Learning Coding Project 3-1: Energy-Based Model\n",
    "\n",
    "Before we start, please put your **Chinese** name and student ID in following format:\n",
    "\n",
    "Name, 0000000000 // e.g.) 傅炜, 2021123123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b760d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e30c80977fd0505d51c0f3fff9f44141",
     "grade": true,
     "grade_id": "name-and-id",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99bd7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46bedcc035ba754f08099a95d00f77f8",
     "grade": false,
     "grade_id": "cell-140b078480df53e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will use Python 3, [NumPy](https://numpy.org/), and [PyTorch](https://pytorch.org/) packages for implementation. This notebook has been tested under the latest stable release version.\n",
    "\n",
    "In this coding project, you will implement 4 generative models, i.e., energy-based model, flow-based model, variational auto-encoder, and generative adverserial network, to generate MNIST images.\n",
    "\n",
    "**We will implement an energy-based model in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cd281",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d53ead84771a754bb0c551fa3317688e",
     "grade": false,
     "grade_id": "cell-ffacdab926dcdef6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In some cells and files you will see code blocks that look like this:\n",
    "\n",
    "```Python\n",
    "##############################################################################\n",
    "#                  TODO: You need to complete the code here                  #\n",
    "##############################################################################\n",
    "raise NotImplementedError()\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################\n",
    "```\n",
    "\n",
    "You should replace `raise NotImplementedError()` with your own implementation based on the context, such as:\n",
    "\n",
    "```Python\n",
    "##############################################################################\n",
    "#                  TODO: You need to complete the code here                  #\n",
    "##############################################################################\n",
    "y = w * x + b\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################\n",
    "\n",
    "```\n",
    "\n",
    "When completing the notebook, please adhere to the following rules:\n",
    "\n",
    "+ Do not write or modify any code outside of code blocks\n",
    "+ Do not add or delete any cells from the notebook.\n",
    "+ Run all cells before submission. We will not re-run the entire codebook during grading.\n",
    "\n",
    "**Finally, avoid plagiarism! Any student who violates academic integrity will be seriously dealt with and receive an F for the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3bbf1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59e39775fb7132b471582e4efedc7243",
     "grade": false,
     "grade_id": "cell-599bd6afccb34d60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task\n",
    "\n",
    "The energy-based method aims to train a parameterized model $E = f(x;\\theta)$ to\n",
    "model the unnormalized data distribution $p(x)\\propto \\exp(-E)$. In this notebook, we instantiate\n",
    "$E = f(x;\\theta)$ as an MLP. Your tasks are as follows:\n",
    "\n",
    "1. **Implement all the missing parts in the contrastive-divergence training pipeline.**\n",
    "\n",
    "Basically, we want to decrease\n",
    "the energy of positive samples while increase the energy of negative samples. The positive samples are from the training set, and the negative\n",
    "samples are sampled using Langevin dynamics starting from either random noise or previously generated samples.\n",
    "\n",
    "2. **Implement an inpainting procedure to recover the original image.**\n",
    "\n",
    "We corrupt the images by adding noise to the pixels in even rows (see\n",
    "below). Please implement an inpainting procedure to recover the original\n",
    "image, then report the\n",
    "mean squared difference between your recovered images and the ground\n",
    "truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8228e39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8faaef554f2902c0d2a24e870f8a80c",
     "grade": false,
     "grade_id": "cell-1c233faf26d419be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# figure size in inches optional\n",
    "rcParams['figure.figsize'] = 11, 8\n",
    "\n",
    "# read images\n",
    "img_A = mpimg.imread('./ebm/groundtruth.png')\n",
    "img_B = mpimg.imread('./ebm/corrupted.png')\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img_A)\n",
    "ax[1].imshow(img_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cb6d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84429f7ac1851d0fbe339858358aaf4a",
     "grade": false,
     "grade_id": "cell-53585dd2f2b21500",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Submission\n",
    "\n",
    "You need to submit your code (this notebook), your trained model (named `./ebm/ebm_best.pth`), and your report:\n",
    "\n",
    "+ **Code**\n",
    "\n",
    "Remember to run all the cells before submission. Remain your tuned hyperparameters unchanged.\n",
    "\n",
    "+ **Model**\n",
    "\n",
    "In this notebook, we select the best model according to the MSE of inpainting. You can also manually test your models and select the best one. **Please do not submit any other checkpoints except for `./ebm/ebm_best.pth`!**\n",
    "\n",
    "+ **Report**\n",
    "\n",
    "Please include inpainting examples and the inpainting MSE on validation set in your\n",
    "report. Note that you only need to write a single report for this coding project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa645f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59d2220da64226d8d0d20aeddc57167c",
     "grade": false,
     "grade_id": "cell-53585dd2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Grading\n",
    "\n",
    "Your implementation will be graded based on **the mean squared error\n",
    "of inpainting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90a8be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "629c76063b8cf64b9021654fbe717d41",
     "grade": false,
     "grade_id": "cell-527c7725f3508ba0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tips\n",
    "\n",
    "+ Training with naive contrastive-divergence algorithm will make your model diverge quickly (think about why). Therefore, you need to add a L2 regularization term $\\alpha(E_\\theta(x+)^2 + E_\\theta(x-)^2)$ to stabilize training.\n",
    "\n",
    "+ Keep track of the generated samples during training to get a sense of how well your model is evolving.\n",
    "\n",
    "+ You can take a look at the paper [Implicit Generation and Generalization in Energy Based Models](https://arxiv.org/pdf/1903.08689.pdf) to learn more about useful tricks to get your model working.\n",
    "\n",
    "+ Make sure your code runs fine with the evaluation cell in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd040c70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54854fcdc1556d353b55bc4d1a133674",
     "grade": false,
     "grade_id": "cell-2b4bea3e44a7b81b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Set Up Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca3738",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bb447e0084709f0b673b1e91ba0aa1a",
     "grade": false,
     "grade_id": "cell-291232b1c59e4f02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you use Colab in this coding project, please uncomment the code, fill the `GOOGLE_DRIVE_PATH_AFTER_MYDRIVE` and run the following cells to mount your Google drive. Then, the notebook can find the required file (i.e., utils.py). If you run the notebook locally, you can skip the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c2354b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b34bc8b23f9c8e480a9671ef3453e7ac",
     "grade": false,
     "grade_id": "cell-a551fcc5ff27fb87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785a7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c62c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cd2080",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d5d6f96e1d7ce5df1315172b57173de",
     "grade": false,
     "grade_id": "cell-e11eaf041d72deda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good luck!\n"
     ]
    }
   ],
   "source": [
    "from utils import hello\n",
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89801ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c559f41bf7c84970b94a49b43e138e98",
     "grade": false,
     "grade_id": "cell-ab2926992ebc021c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, please run the following cell to import some base classes for implementation (no matter whether you use colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf1d5cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc6d9d087b7f8aedc3401222f717bb07",
     "grade": false,
     "grade_id": "cell-c0b91f0d2b7ecc80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy\\AppData\\Local\\Temp\\ipykernel_2772\\3093347763.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from utils import save_model, load_model, corruption, train_set, val_set\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "os.makedirs('./ebm', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b76802",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbb5921ccbc0e1e14fd40df09afebcac",
     "grade": false,
     "grade_id": "cell-f28aaf301b501410",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## MLP Model\n",
    "\n",
    "We have provided an example MLP implementation. Feel free to modify the following cell the implement your own model.\n",
    "\n",
    "**Note that your model should be an MLP!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ca4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MlpBackbone(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_size, activation=F.elu):\n",
    "        super(MlpBackbone, self).__init__()\n",
    "        self.input_shape = input_shape  # (C, H, W)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(np.prod(self.input_shape), self.hidden_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.hidden_size, int(self.hidden_size/2))\n",
    "\n",
    "        self.fc3 = nn.Linear(int(self.hidden_size/2), int(self.hidden_size/4))\n",
    "\n",
    "        #self.fc4 = nn.Linear(int(self.hidden_size/2), int(self.hidden_size/4))\n",
    "\n",
    "        self.fc5 = nn.Linear(int(self.hidden_size/4), 1)  # Output layer\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten input\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        #x = self.fc4(x)\n",
    "        #x = self.activation(x)\n",
    "\n",
    "        out = self.fc5(x)                   # Output layer (no BatchNorm or activation)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61159f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd456d1555c954ff15d4cfcb44b144f5",
     "grade": false,
     "grade_id": "cell-cafb02a0cc941c58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Implement Langevin dynamics in the following cell. Pay attention to the gradients of both your energy model and input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b704099a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91d2744d6a964f70ff2c85c161f92c9f",
     "grade": false,
     "grade_id": "cell-afd44e44fcd9d650",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def langevin_step(energy_model, x, step_lr=10, eps=0.005, max_grad_norm=0.03, steps=1, annealing=False):\n",
    "    \"\"\"\n",
    "    Perform multiple steps of Langevin dynamics sampling.\n",
    "\n",
    "    Args:\n",
    "        energy_model (nn.Module): The energy-based model used for sampling.\n",
    "        x (torch.Tensor): The input tensor to update via Langevin dynamics.\n",
    "        step_lr (float): The learning rate of the optimizer used to update the input.\n",
    "        eps (float): The step size of the Langevin dynamics update.\n",
    "        max_grad_norm (float or None): The maximum norm of the gradient for gradient clipping.\n",
    "        steps (int): The number of Langevin dynamics steps to perform.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The updated input tensor after multiple steps of Langevin dynamics.\n",
    "    \"\"\"\n",
    "    # Set the energy model to evaluation mode and disable gradient\n",
    "    is_training = energy_model.training\n",
    "    energy_model.eval()\n",
    "    x = x.detach().clone().requires_grad_(True)\n",
    "    lr_now = step_lr\n",
    "    eps_now = eps\n",
    "    for p in energy_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # Compute the gradient of the energy w.r.t. the input tensor\n",
    "        energy = energy_model(x)\n",
    "        energy.backward(torch.ones_like(energy))\n",
    "        grad = x.grad\n",
    "\n",
    "        if annealing:\n",
    "            lr_now = step_lr * (0.5 + 0.25*(1 + np.cos(np.pi * step / steps)))\n",
    "            eps_now = eps * np.sqrt(0.1 + 0.45*(1 + np.cos(np.pi * step / steps)))\n",
    "        # Optionally clip the gradient norm\n",
    "        if max_grad_norm is not None:\n",
    "            grad_norm = grad.norm(p=2)\n",
    "            grad = grad * torch.clamp(grad_norm / max_grad_norm, max=1.0)\n",
    "        \n",
    "        # Update the input tensor using Langevin dynamics\n",
    "        noise = eps_now * torch.randn_like(x)  # Add noise for stochasticity\n",
    "        x = x - lr_now * grad + noise\n",
    "        \n",
    "        # Clamp values to [0,1] range since these are image pixels\n",
    "        x = torch.clamp(x, 0, 1)\n",
    "        \n",
    "        # Reset gradients for the input tensor\n",
    "        x = x.detach().clone().requires_grad_(True)\n",
    "    \n",
    "    for p in energy_model.parameters():\n",
    "        p.requires_grad = True\n",
    "    energy_model.train(is_training)\n",
    "    \n",
    "    return x.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1fdbf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5171faedb56d4b5f97620596006e4525",
     "grade": false,
     "grade_id": "cell-508a10378fa57a85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Inpainting\n",
    "\n",
    "Implement the inpainting procedure. Think about the difference between sampling and inpainting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7ed1856",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15d3e13b437efa0e295540e214b3a612",
     "grade": false,
     "grade_id": "cell-5fce3238f7ff50db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim_fn\n",
    "\n",
    "def inpainting(\n",
    "    energy_model, \n",
    "    x, \n",
    "    mask, \n",
    "    n_steps, \n",
    "    step_lr, \n",
    "    max_grad_norm, \n",
    "    max_noise=0.005,\n",
    "    momentum=0.9, \n",
    "    use_momentum=False, \n",
    "    lr_schedule='cosine',\n",
    "    adaptive_step=False,\n",
    "    adapt_lr_factor=1.0,\n",
    "    adapt_decay=0.9,\n",
    "    eps_min=1e-6,\n",
    "    early_stopping=True,\n",
    "    tol=1e-4,\n",
    "    patience=10,\n",
    "    metric='ssim'\n",
    "):\n",
    "    \"\"\"\n",
    "    Inpainting function that completes an image given a masked input using Langevin\n",
    "    dynamics with optional momentum, scheduling, adaptive step size, adaptive noise,\n",
    "    and early stopping.\n",
    "\n",
    "    Args:\n",
    "        energy_model (nn.Module): The energy-based model used to generate the image.\n",
    "        x (torch.Tensor): Masked image tensor to be completed.\n",
    "        mask (torch.Tensor): Binary mask tensor (1 for observed pixel, 0 for missing).\n",
    "        n_steps (int): Maximum number of Langevin steps to run.\n",
    "        step_lr (float): Base step size (learning rate) for updates.\n",
    "        max_grad_norm (float or None): If not None, clip gradient norms to this value.\n",
    "        max_noise (float): Base noise amplitude for Langevin updates.\n",
    "        momentum (float): Momentum factor for heavy-ball updates.\n",
    "        use_momentum (bool): Whether to use momentum-based updates.\n",
    "        lr_schedule (str): One of ['cosine', 'fixed', 'linear'] for scheduling step size/noise.\n",
    "        adaptive_step (bool): Whether to adapt step size (and noise) based on gradient norms.\n",
    "        adapt_lr_factor (float): Scaling factor for the adaptive step size.\n",
    "        adapt_decay (float): EMA decay for running average of gradient norms.\n",
    "        eps_min (float): Minimum value added to denominator (prevents division by zero).\n",
    "        early_stopping (bool): If True, stops early if changes in the image are below `tol`.\n",
    "        tol (float): Tolerance for the change in the image to consider convergence.\n",
    "        patience (int): Number of consecutive steps with change below `tol` before stopping.\n",
    "        metric (str): 'l2' or 'ssim' for measuring change in early stopping.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The completed (inpainted) image tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- PREPARE THE MODEL FOR EVALUATION ---\n",
    "    was_training = energy_model.training\n",
    "    energy_model.eval()\n",
    "    for param in energy_model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # --- INITIAL SETUP ---\n",
    "    x = x.clone()  # Copy to avoid modifying the original\n",
    "    device = x.device\n",
    "\n",
    "    # Momentum velocity\n",
    "    if use_momentum:\n",
    "        velocity = torch.zeros_like(x)\n",
    "    else:\n",
    "        velocity = 0\n",
    "\n",
    "    # Running average of gradient norms for adaptive step/noise\n",
    "    if adaptive_step:\n",
    "        running_avg_grad_norm = torch.tensor(1.0, device=device)  # avoid div by zero at init\n",
    "\n",
    "    # Variables for early stopping\n",
    "    if early_stopping:\n",
    "        prev_x = x.clone()\n",
    "        stop_counter = 0\n",
    "\n",
    "    # --- HELPER FUNCTION FOR SCHEDULING ---\n",
    "    def get_scheduled_val(step, total_steps, base_val, schedule_type):\n",
    "        \"\"\"\n",
    "        Return a scaled value for either step size or noise, depending on schedule_type.\n",
    "        \"\"\"\n",
    "        alpha = float(step) / float(total_steps)\n",
    "        if schedule_type == \"cosine\":\n",
    "            # Cosine schedule from base_val to 0.2 * base_val\n",
    "            scale = 0.2 + 0.8 * (0.5 * (1.0 + np.cos(np.pi * alpha)))\n",
    "            return base_val * scale\n",
    "        elif schedule_type == \"linear\":\n",
    "            return base_val * (1.0 - alpha)\n",
    "        else:  # 'fixed'\n",
    "            return base_val\n",
    "\n",
    "    # --- MAIN LANGEVIN LOOP ---\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        # 1. DETACH/REQUIRES_GRAD\n",
    "        x_detach = x.detach().clone().requires_grad_(True)\n",
    "\n",
    "        # 2. FORWARD AND BACKWARD\n",
    "        energy = energy_model(x_detach)\n",
    "        energy.backward(torch.ones_like(energy))\n",
    "        grad = x_detach.grad\n",
    "\n",
    "        # 3. GRADIENT CLIPPING\n",
    "        if max_grad_norm is not None:\n",
    "            grad_norm = grad.norm(p=2)\n",
    "            if grad_norm > max_grad_norm:\n",
    "                grad = grad * (max_grad_norm / grad_norm)\n",
    "        else:\n",
    "            grad_norm = grad.norm(p=2)\n",
    "\n",
    "        # 4. ADAPTIVE STEP AND NOISE (IF ENABLED)\n",
    "        if adaptive_step:\n",
    "            # Update the running average of gradient norms\n",
    "            running_avg_grad_norm = adapt_decay * running_avg_grad_norm + \\\n",
    "                                    (1 - adapt_decay) * grad_norm\n",
    "\n",
    "            # Inversely scale step size by the running average grad norm\n",
    "            step_lr_now = adapt_lr_factor / (running_avg_grad_norm + eps_min)\n",
    "            \n",
    "            # Inversely scale noise by running average grad norm\n",
    "            # (If you prefer noise to remain fixed, you can skip this.)\n",
    "            noise_now = max_noise / (running_avg_grad_norm + eps_min)\n",
    "            noise_now = torch.clamp(noise_now, min=eps_min)\n",
    "\n",
    "        else:\n",
    "            # Use scheduled (or fixed) step sizes\n",
    "            step_lr_now = get_scheduled_val(step, n_steps, step_lr, lr_schedule)\n",
    "            noise_now   = get_scheduled_val(step, n_steps, max_noise, lr_schedule)\n",
    "\n",
    "        # 5. LANGEVIN UPDATE\n",
    "        langevin_noise = noise_now * torch.randn_like(x)\n",
    "        if use_momentum:\n",
    "            velocity = momentum * velocity - step_lr_now * grad + langevin_noise\n",
    "            x = x + velocity\n",
    "        else:\n",
    "            x = x - step_lr_now * grad + langevin_noise\n",
    "\n",
    "        # Enforce pixel range [0,1] (if desired)\n",
    "        x = torch.clamp(x, 0, 1)\n",
    "\n",
    "        # Preserve the observed pixels (mask=1) while updating only the missing ones (mask=0)\n",
    "        x = mask * x.detach() + (1 - mask) * x\n",
    "\n",
    "        # 6. EARLY STOPPING CHECK\n",
    "        if early_stopping:\n",
    "            if metric == 'l2':\n",
    "                change = torch.norm(x - prev_x).item()\n",
    "            elif metric == 'ssim':\n",
    "                # Note that SSIM is 1.0 if the images are identical, so we measure 1 - ssim.\n",
    "                change = 1.0 - ssim_fn(x, prev_x, data_range=1.0).item()\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric for early stopping. Use 'l2' or 'ssim'.\")\n",
    "\n",
    "            if change < tol:\n",
    "                stop_counter += 1\n",
    "                if stop_counter >= patience:\n",
    "                    print(f\"Early stopping at step {step + 1}/{n_steps} with change={change:.6f}.\")\n",
    "                    break\n",
    "            else:\n",
    "                stop_counter = 0\n",
    "\n",
    "            prev_x = x.clone()\n",
    "\n",
    "    # --- RESTORE MODEL STATE ---\n",
    "    for param in energy_model.parameters():\n",
    "        param.requires_grad_(True)\n",
    "    energy_model.train(was_training)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593bd650",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6153ff029424a3266efd4b44329a1223",
     "grade": false,
     "grade_id": "cell-e4f7157f3a2421d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(energy_model, val_loader, n_sample_steps, step_lr, langevin_grad_norm, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the energy model on the validation set and returns the corruption MSE,\n",
    "    recovered MSE, corrupted images, and recovered images for visualization.\n",
    "\n",
    "    Args:\n",
    "        energy_model (nn.Module): Trained energy-based model.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation data loader.\n",
    "        n_sample_steps (int): Number of Langevin dynamics steps to take when sampling.\n",
    "        step_lr (float): Learning rate to use during Langevin dynamics.\n",
    "        langevin_grad_norm (float): Maximum L2 norm of the Langevin dynamics gradient.\n",
    "        device (str): Device to use (default='cuda').\n",
    "    \"\"\"\n",
    "    mse = corruption_mse = 0\n",
    "    energy_before_sampling = energy_after_sampling = 0\n",
    "    n_batches = 0\n",
    "    energy_model.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(val_loader.dataset))\n",
    "    pbar.set_description('Eval')\n",
    "    for data, _ in val_loader:\n",
    "        n_batches += data.shape[0]\n",
    "        data = data.to(device)\n",
    "        broken_data, mask = corruption(data, type_='ebm')\n",
    "        energy_before_sampling += energy_model(broken_data).sum().item()\n",
    "        recovered_img = inpainting(energy_model, broken_data, mask,\n",
    "                                   n_sample_steps, step_lr, langevin_grad_norm)\n",
    "        energy_after_sampling += energy_model(recovered_img).sum().item()\n",
    "\n",
    "        mse += np.mean((data.detach().cpu().numpy().reshape(-1, 28 * 28) - recovered_img.detach().cpu().numpy().reshape(-1, 28 * 28)) ** 2, -1).sum().item()\n",
    "        corruption_mse += np.mean((data.detach().cpu().numpy().reshape(-1, 28 * 28) - broken_data.detach().cpu().numpy().reshape(-1, 28 * 28)) ** 2, -1).sum().item()\n",
    "\n",
    "        pbar.update(data.shape[0])\n",
    "        pbar.set_description('Corruption MSE: {:.6f}, Recovered MSE: {:.6f}, Energy Before Sampling: {:.6f}, Energy After Sampling: {:.6f}'.format(\n",
    "            corruption_mse / n_batches, mse / n_batches, energy_before_sampling / n_batches, energy_after_sampling / n_batches))\n",
    "\n",
    "    pbar.close()\n",
    "    return (corruption_mse / n_batches, mse / n_batches, data[:100].detach().cpu(), broken_data[:100].detach().cpu(), recovered_img[:100].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da520b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample_gen (energy_model, buffer_size, replay_buffer, x_plus, batchsize, replay_ratio, n_steps, step_lr=10, eps=0.005, max_grad_norm=0.03, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate negative samples using Langevin dynamics with a replay buffer.\n",
    "\n",
    "    Args:\n",
    "        energy_model (nn.Module): The energy-based model used for sampling.\n",
    "        buffer_size (int): The size of the replay buffer.\n",
    "        replay_buffer (torch.Tensor): The replay buffer containing samples for negative sampling.\n",
    "        x_plus (torch.Tensor): The positive samples to generate negative samples for.\n",
    "        batchsize (int): The batch size for generating negative samples.\n",
    "        replay_ratio (float): The probability of sampling from the replay buffer.\n",
    "        n_steps (int): The number of steps of Langevin dynamics to run.\n",
    "        step_lr (float): The step size of Langevin dynamics.\n",
    "        eps (float): The step size of Langevin dynamics.\n",
    "        max_grad_norm (float): The maximum gradient norm to be used for gradient clipping.\n",
    "        device (str): Device to use (default='cuda').\n",
    "\n",
    "    Returns:\n",
    "        x_minus (torch.Tensor): The generated negative samples.\n",
    "        energy_before (float): The energy of the negative samples before Langevin dynamics.\n",
    "    \"\"\"\n",
    "    is_training = energy_model.training\n",
    "    energy_model.eval()\n",
    "    energy_before = 0\n",
    "\n",
    "    if buffer_size == 0:\n",
    "        x_minus = torch.rand_like(x_plus)\n",
    "    else:\n",
    "        buffer_indices = torch.randint(0, buffer_size, (batchsize,))\n",
    "        x_replay = replay_buffer[buffer_indices].to(device)\n",
    "        random_samples = torch.rand_like(x_plus).to(device)\n",
    "        mask = torch.rand(batchsize, 1, 1, 1, device=device) < replay_ratio\n",
    "        x_minus = torch.where(mask, x_replay, random_samples)\n",
    "    x_minus = x_minus.to(device)\n",
    "\n",
    "    energy_before += energy_model(x_minus).sum().item()\n",
    "   \n",
    "    x_minus = langevin_step(energy_model, x_minus, step_lr, eps, max_grad_norm, n_steps)\n",
    "    \n",
    "    energy_model.train(is_training)\n",
    "    \n",
    "    return x_minus, energy_before\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62fbcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "408288173fbc37fcc2d75c0057690de4",
     "grade": false,
     "grade_id": "cell-bfaee9e8110d2da5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training\n",
    "Fill the missing parts in the `train` function. There are some comments implying what to do in the corresponding blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02bdcb85",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53e89f05699b12aa2cb1748b4e0b43b7",
     "grade": false,
     "grade_id": "cell-e72fcecf0e87cf84",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, energy_model, train_loader, val_loader, optimizer, n_sample_steps, step_lr, langevin_eps, l2_alpha, langevin_grad_norm =None,\n",
    "          device='cuda', buffer_maxsize=int(1e5), replay_ratio=0.95, save_interval=1):\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    energy_model.to(device)\n",
    "    replay_buffer = torch.zeros(buffer_maxsize, 1, 28, 28)\n",
    "    buffer_size = buffer_ptr = 0\n",
    "    best_mse = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = energy_before = energy_plus = energy_minus = n_batches = 0\n",
    "        pbar = tqdm(total=len(train_loader.dataset))\n",
    "        pbar.set_description('Train')\n",
    "        for i, (x_plus, _) in enumerate(train_loader):\n",
    "            n_batches += x_plus.shape[0]\n",
    "            bs = x_plus.shape[0]\n",
    "\n",
    "             # init negative samples\n",
    "            if buffer_size == 0:\n",
    "                x_minus = torch.rand_like(x_plus)\n",
    "            else:\n",
    "                ############################### CODE HERE ######################################\n",
    "                buffer_indices = torch.randint(0, buffer_size, (bs,))\n",
    "                x_replay = replay_buffer[buffer_indices].to(device)\n",
    "                random_samples = torch.rand_like(x_plus).to(device)\n",
    "                mask = torch.rand(bs, 1, 1, 1, device=device) < replay_ratio\n",
    "                x_minus = torch.where(mask, x_replay, random_samples)\n",
    "                ##############################################################################\n",
    "            x_minus = x_minus.to(device)\n",
    "\n",
    "            energy_before += energy_model(x_minus).sum().item()\n",
    "\n",
    "            # sample negative samples\n",
    "            ################################# CODE HERE ##################################\n",
    "            # YOUR CODE HERE\n",
    "            x_minus = langevin_step(energy_model, x_minus, step_lr, langevin_eps, langevin_grad_norm, n_sample_steps,True)\n",
    "                \n",
    "            # extend buffer\n",
    "            if buffer_ptr + bs <= buffer_maxsize:\n",
    "                replay_buffer[buffer_ptr: buffer_ptr +\n",
    "                              bs] = ((x_minus * 255).to(torch.uint8).float() / 255).cpu()\n",
    "            else:\n",
    "                x_minus_ = (\n",
    "                    (x_minus * 255).to(torch.uint8).float() / 255).cpu()\n",
    "                replay_buffer[buffer_ptr:] = x_minus_[\n",
    "                    :buffer_maxsize - buffer_ptr]\n",
    "                remaining = bs - (buffer_maxsize - buffer_ptr)\n",
    "                replay_buffer[:remaining] = x_minus_[\n",
    "                    buffer_maxsize - buffer_ptr:]\n",
    "            buffer_ptr = (buffer_ptr + bs) % buffer_maxsize\n",
    "            buffer_size = min(buffer_maxsize, buffer_size + bs)\n",
    "\n",
    "            # compute loss\n",
    "            energy_model.train()\n",
    "            x_plus = x_plus.to(device)\n",
    "            x_minus = x_minus.to(device)\n",
    "            e_plus = energy_model(x_plus)\n",
    "            e_minus = energy_model(x_minus)\n",
    "            \n",
    "            ##############################################################################\n",
    "            l2_regularization = l2_alpha * ((e_plus ** 2).mean() + (e_minus ** 2).mean()) \n",
    "            loss = e_plus.mean() - e_minus.mean() + l2_regularization\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ##############################################################################\n",
    "\n",
    "            train_loss += loss.sum().item()\n",
    "            energy_plus += e_plus.sum().item()\n",
    "            energy_minus += e_minus.sum().item()\n",
    "\n",
    "            pbar.update(x_plus.size(0))\n",
    "            pbar.set_description(\"Train Epoch {}, Train Loss: {:.6f}, \".format(epoch + 1, 100*train_loss / n_batches) +\n",
    "                                 \"Energy Before Sampling: {:.6f}, \".format(energy_before / n_batches) +\n",
    "                                 \"Energy After Sampling: {:.6f}, \".format(energy_minus / n_batches) +\n",
    "                                 \"Energy of Ground Truth: {:.6f}\".format(energy_plus / n_batches))\n",
    "        pbar.close()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            os.makedirs(f'./ebm/{epoch + 1}', exist_ok=True)\n",
    "            energy_model.eval()\n",
    "            save_model(f'./ebm/{epoch + 1}/ebm.pth',\n",
    "                       energy_model, optimizer, replay_buffer)\n",
    "\n",
    "            # evaluate inpainting\n",
    "            # feel free to change the inpainting parameters!\n",
    "            c_mse, r_mse, original, broken, recovered = evaluate(energy_model, val_loader,\n",
    "                                                                 500, 10, 0.03, device=device)\n",
    "            torchvision.utils.save_image(\n",
    "                original, f\"./ebm/{epoch + 1}/groundtruth.png\", nrow=10)\n",
    "            torchvision.utils.save_image(\n",
    "                broken, f\"./ebm/{epoch + 1}/corrupted.png\", nrow=10)\n",
    "            torchvision.utils.save_image(\n",
    "                recovered, f\"./ebm/{epoch + 1}/recovered.png\", nrow=10)\n",
    "            if r_mse < best_mse:\n",
    "                print(f'Current best MSE: {best_mse} -> {r_mse}')\n",
    "                best_mse = r_mse\n",
    "                save_model('./ebm/ebm_best.pth', energy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "334975fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MlpBackbone((1, 28, 28), 512).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0, 0.999))\n",
    "\n",
    "train_loader = DataLoader(train_set, 512, shuffle=True, drop_last=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, 2000, shuffle=True, drop_last=False, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150ba25",
   "metadata": {},
   "source": [
    "Now you can start your training. Please keep in mind that this cell may **NOT** be run when we evaluate your assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free the change training hyper-parameters!\n",
    "\n",
    "train(n_epochs=50, energy_model=model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer,\n",
    "       n_sample_steps=200, step_lr=10, langevin_eps=0.005, langevin_grad_norm= 0.03, l2_alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d9ad1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "346cba624d53af3f3fed53aab993f261",
     "grade": false,
     "grade_id": "cell-c20c624e7ec0d633",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Make sure you can run the following evaluation cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4ed12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to change evaluation parameters!\n",
    "# inpainting parameters are not necessarily the same as sampling parameters\n",
    "n_sample_steps = 3000\n",
    "step_lr = 10\n",
    "langevin_grad_norm = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "988fa737",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a52957140b64eb164a8f5bc3ecee0a5c",
     "grade": false,
     "grade_id": "cell-a3504d53a9a04b33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\课程\\大二下\\深度学习\\CodingProject3\\utils.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(load_path)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439e18265a5a4ee0909c701b94f25723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption MSE: 0.06340540924072266\n",
      "Recovered MSE: 0.04328401107788086\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(load_model('./ebm/ebm_best.pth')[0])\n",
    "corruption_mse, mse, original_eval, broken_eval, recovered_eval = evaluate(model, val_loader, n_sample_steps, step_lr, langevin_grad_norm, device=device)\n",
    "print(f'Corruption MSE: {corruption_mse}')\n",
    "print(f'Recovered MSE: {mse}')\n",
    "os.makedirs(f'./ebm/eval', exist_ok=True)\n",
    "torchvision.utils.save_image(\n",
    "                original_eval, f\"./ebm/eval/groundtruth.png\", nrow=10)\n",
    "torchvision.utils.save_image(\n",
    "                broken_eval, f\"./ebm/eval/corrupted.png\", nrow=10)\n",
    "torchvision.utils.save_image(\n",
    "                recovered_eval, f\"./ebm/eval/recovered.png\", nrow=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
